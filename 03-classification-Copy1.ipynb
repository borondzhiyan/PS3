{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28594d14f19a6684",
   "metadata": {},
   "source": [
    "# Assignment 3: Classification\n",
    "\n",
    "**Deadline**: 28 April 2025\n",
    "\n",
    "**Packages**: NumPy, Pandas, Scikit-learn\n",
    "\n",
    "**Name**: Sergey Bororndzhiyan\n",
    "\n",
    "**Matriculation Number**: K12353745\n",
    "\n",
    "**Submission Instructions**: Upload your Jupyter notebook and a PDF export (with results) on Moodle. Furthermore, upload the exported KNIME workflow (with your pickle file in the data directory) for Task 1.4 on Moodle. Go to the corresponding checkmark list to indicate which tasks you have completed and feel confident to explain in class. The checkmark list will be the basis for grading. If you fail to explain your submission you will be awarded 0 points for the entire assignment; after the second such incident you would fail the course.\n",
    "\n",
    "Write your solutions in the code cells for the different tasks. You may also add additional code cells as well as markdown cells if you want to write down additional explanations, observations, or assumptions.\n",
    "\n",
    "There are also questions that require you to write textual answers into markdown cells.\n",
    "\n",
    "**If anything is unclear, ask in the forum on Moodle and/or make reasonable assumptions. Document any such assumptions in the Jupyter notebook and the PDF report.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6665a8a59d5e2f7",
   "metadata": {},
   "source": [
    "## Case 1: Bank Loans\n",
    "\n",
    "**Files**: loans.csv\n",
    "\n",
    "You have a dataset of loan documentation. The goal is to train a model that will predict whether a loan is good or bad based on various indicators."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfac13b4280ab62",
   "metadata": {},
   "source": [
    "### Task 1.1: Data Preprocessing\n",
    "\n",
    "Our analysis will require a class variable to distinguish good loans from bad loans. We can derive that from the detailed loan status described by the *loan_status* variable. Add a variable *class*, derived from *loan_status* as follows:\n",
    "\n",
    "* Assign *class* to 'good' if *loan_status* is 'Fully Paid' or 'Does not meet the credit policy. Status:Fully Paid'.\n",
    "* Assign *class* to 'bad' if *loan_status* is 'Default', 'Charged Off', or 'Does not meet the credit policy. Status:Charged Off'.\n",
    "\n",
    "Our analysis will require that each loan's class is known. Include only inactive loans for which the class is known, which are loans with a *loan_status* value mapped to class 'good' or 'bad'. Other *loan_status* values indicate that a loan is still active.\n",
    "\n",
    "Our analysis will require variables to be in numerical representation, though the dataset includes potentially useful information in categorical representation. Convert categorical variables to numerical representation as follows:\n",
    "\n",
    "* From *term*, remove the word 'months'.\n",
    "* From *emp_length*, remove 'year*', change '< 1' to 0, change '10+' to 10, change 'n/a' to null value.\n",
    "* Change *grade*, *sub_grade*, *home_ownership*, and *purpose* to numerical values by index coding, i.e., transforming the string categories into numeric values, where each category corresponds to an integer value.\n",
    "\n",
    "Transform the dataset for analysis. Filter out (i.e., remove) some of the variables like this:\n",
    "\n",
    "* Identification variables, like *id* and *member_id* are not predictive, so we do not include them.\n",
    "* Leaky variables are those that contain information that could only be known when the class is already known. Since we are ultimately interested in predicting the class before the class is actually known, we do not include leaky variables *recoveries*, *collection_recovery_fee*, or *collections_12_mths_ex_med*.\n",
    "* Empty variables, i.e., columns with only null values, are missing any information at all, so we do not include them.\n",
    "* No-variance variables are missing any information at all, so we do not include them.\n",
    "* Sparse variables, i.e., those with more than half of their values missing, might be too difficult to sensibly impute, so we do not include them.\n",
    "\n",
    "For convenience, do not include non-numerical variables, except for the class variable.\n",
    "\n",
    "Impute by simply substituting zeros for missing values.\n",
    "\n",
    "Normalize the variables using z-score normalization.\n",
    "\n",
    "Convert to principal component representation and filter out the low-variance principal components. Ultimately, you should end up with only the first two principal components (and the class).\n",
    "\n",
    "See https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html for more information on principal component analysis (PCA) using scikit-learn.\n",
    "\n",
    "The provided KNIME workflow specifies the necessary preprocessing steps. If you double-click on the **Data Cleaning** and **Apply PCA** nodes, you can view the workflow of these components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d0e6bd6cdcc5ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Fully Paid' 'Charged Off' 'Current' 'Default' 'Late (31-120 days)'\n",
      " 'In Grace Period' 'Late (16-30 days)'\n",
      " 'Does not meet the credit policy. Status:Fully Paid'\n",
      " 'Does not meet the credit policy. Status:Charged Off']\n",
      "class\n",
      "good    49561\n",
      "bad     10274\n",
      "Name: count, dtype: int64\n",
      "Original shape : (59835, 35)\n",
      "PCA shape      : (59835, 2)\n",
      "\n",
      "PC   indiv%   cum%\n",
      "01   24.19%   24.19%\n",
      "02    9.62%   33.81%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd            # Library for data manipulation\n",
    "import numpy as np             # Library for numerical operations\n",
    "from sklearn.preprocessing import StandardScaler  # For feature scaling\n",
    "from sklearn.decomposition import PCA              # For principal component analysis\n",
    "import matplotlib.pyplot as plt                    # For plotting\n",
    "\n",
    "\n",
    "loans_df = pd.read_csv(\"loans.csv\") \n",
    "\n",
    "print(loans_df['loan_status'].unique())  # Print all unique statuses\n",
    "\n",
    "# Filter dataset to keep only completed loans\n",
    "completed_statuses = [\n",
    "    'Fully Paid',\n",
    "    'Default',\n",
    "    'Charged Off',\n",
    "    'Does not meet the credit policy. Status:Fully Paid',\n",
    "    'Does not meet the credit policy. Status:Charged Off'\n",
    "]\n",
    "\n",
    "# Keep only loans with final statuses (no active loans)\n",
    "loans_df = loans_df[loans_df['loan_status'].isin(completed_statuses)].copy()\n",
    "\n",
    "# Map loan statuses into 'good' and 'bad'\n",
    "\n",
    "conditions = [\n",
    "    loans_df['loan_status'].isin(['Default', 'Charged Off',\n",
    "                                  'Does not meet the credit policy. Status:Charged Off']),\n",
    "    loans_df['loan_status'].isin(['Fully Paid',\n",
    "                                  'Does not meet the credit policy. Status:Fully Paid'])\n",
    "]\n",
    "choices = ['bad', 'good']\n",
    "\n",
    "# Create new binary target column: 'class'\n",
    "loans_df['class'] = np.select(conditions, choices, default='active')\n",
    "\n",
    "# Preprocess categorical columns\n",
    "\n",
    "# Clean 'term' column: remove \"months\" and convert to integer\n",
    "loans_df['term'] = loans_df['term'].str.replace('months', '', regex=False).str.strip().astype(int)\n",
    "\n",
    "# Clean 'emp_length' column: convert employment length into numeric\n",
    "loans_df['emp_length'] = (loans_df['emp_length']\n",
    "    .str.lower()\n",
    "    .str.replace('years?', '', regex=True)\n",
    "    .str.replace('< 1', '0', regex=False)\n",
    "    .str.replace('10+', '10', regex=False)\n",
    "    .str.strip()\n",
    "    .replace('n/a', np.nan)\n",
    "    .astype(float) \n",
    ")\n",
    "\n",
    "# Encode categorical columns into numeric codes\n",
    "loans_df['grade'] = pd.Categorical(loans_df['grade']).codes\n",
    "loans_df['sub_grade'] = pd.Categorical(loans_df['sub_grade']).codes\n",
    "loans_df['home_ownership'] = pd.Categorical(loans_df['home_ownership']).codes\n",
    "loans_df['purpose'] = pd.Categorical(loans_df['purpose']).codes\n",
    "\n",
    "# Drop irrelevant or problematic columns\n",
    "\n",
    "# Identification variables (useless for prediction)\n",
    "ident_vars = ['id', 'member_id', 'url', 'zip_code']\n",
    "loans_df = loans_df.drop(columns=[col for col in ident_vars if col in loans_df.columns])\n",
    "\n",
    "# \"Leaky\" variables (contain information only available AFTER loan status is known)\n",
    "leaky_vars = ['recoveries', 'collection_recovery_fee', 'collections_12_mths_ex_med']\n",
    "loans_df = loans_df.drop(columns=[col for col in leaky_vars if col in loans_df.columns])\n",
    "\n",
    "# Drop columns that are completely empty\n",
    "empty_vars = [col for col in loans_df.columns if loans_df[col].isnull().all()]\n",
    "loans_df = loans_df.drop(columns=empty_vars)\n",
    "\n",
    "# Drop columns with no variance (constant value)\n",
    "no_variance_vars = [col for col in loans_df.columns\n",
    "    if col != 'class' and loans_df[col].nunique(dropna=True) <= 1]\n",
    "loans_df = loans_df.drop(columns=no_variance_vars)\n",
    "\n",
    "# Keep only numeric columns + 'class'\n",
    "\n",
    "if 'class' in loans_df.columns:\n",
    "    class_series = loans_df['class']\n",
    "    data_numeric = loans_df.select_dtypes(include=[np.number])  # Select numeric columns\n",
    "    loans_df = pd.concat([data_numeric, class_series], axis=1)\n",
    "else:\n",
    "    loans_df = loans_df.select_dtypes(include=[np.number])\n",
    "\n",
    "# Fill missing values\n",
    "\n",
    "# Replace all remaining NaN with 0\n",
    "loans_df = loans_df.fillna(0)\n",
    "\n",
    "# Check class balance\n",
    "print(loans_df['class'].value_counts())\n",
    "\n",
    "# Split features and target-\n",
    "\n",
    "# X: feature matrix (exclude 'class' column)\n",
    "X = loans_df.drop(columns=['class'])\n",
    "\n",
    "# y: target column ('class')\n",
    "y = loans_df['class']\n",
    "\n",
    "# Standardize (scale) features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)  # Features are now centered and scaled (mean=0, std=1)\n",
    "\n",
    "\n",
    "# Apply PCA (Principal Component Analysis)\n",
    "\n",
    "# Reduce dimensionality to 2 components\n",
    "pca = PCA(n_components=2, svd_solver='full')\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Create a DataFrame with PC1 and PC2\n",
    "X_pca_df = pd.DataFrame(X_pca, columns=['PC1', 'PC2'], index=X.index)\n",
    "\n",
    "\n",
    "print(f\"Original shape : {X.shape}\")      # (number of samples, number of original features)\n",
    "print(f\"PCA shape      : {X_pca.shape}\")  # (number of samples, 2)\n",
    "\n",
    "# Explained variance\n",
    "indiv = pca.explained_variance_ratio_  # Variance explained by each PC\n",
    "cum = indiv.cumsum()                   # Cumulative variance explained\n",
    "\n",
    "print(\"\\nPC   indiv%   cum%\")\n",
    "for k, (iv, cv) in enumerate(zip(indiv, cum), start=1):\n",
    "    print(f\"{k:02d}   {iv:6.2%}   {cv:6.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f9b82db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PC1: 24.191%\n",
      "PC2: 9.622%\n",
      "PC3: 6.654%\n",
      "PC4: 5.720%\n",
      "PC5: 5.169%\n",
      "PC6: 4.132%\n",
      "PC7: 3.884%\n",
      "PC8: 3.630%\n",
      "PC9: 3.241%\n",
      "PC10: 3.119%\n",
      "PC11: 2.964%\n",
      "PC12: 2.791%\n",
      "PC13: 2.724%\n",
      "PC14: 2.613%\n",
      "PC15: 2.540%\n",
      "PC16: 2.360%\n",
      "PC17: 2.107%\n",
      "PC18: 1.913%\n",
      "PC19: 1.828%\n",
      "PC20: 1.762%\n",
      "PC21: 1.586%\n",
      "PC22: 1.307%\n",
      "PC23: 1.188%\n",
      "PC24: 0.767%\n",
      "PC25: 0.757%\n",
      "PC26: 0.493%\n",
      "PC27: 0.368%\n",
      "PC28: 0.265%\n",
      "PC29: 0.159%\n",
      "PC30: 0.055%\n",
      "PC31: 0.051%\n",
      "PC32: 0.023%\n",
      "PC33: 0.011%\n",
      "PC34: 0.005%\n",
      "PC35: 0.000%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# X – матрица признаков без целевой переменной (например, loans_df.drop('class', axis=1))\n",
    "scaler = StandardScaler()\n",
    "X_z = scaler.fit_transform(X)        # fit = вычислить µ,σ; transform = применить\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# 1) сколько компонентов оставить? None → сохраняем ВСЕ\n",
    "pca = PCA(n_components=None, svd_solver='full')\n",
    "Z = pca.fit_transform(X_z)\n",
    "\n",
    "# Доля объяснённой дисперсии по компонентам\n",
    "explained = pca.explained_variance_ratio_\n",
    "\n",
    "# Посмотрим распределение дисперсий\n",
    "for i, var in enumerate(explained, start=1):\n",
    "    print(f'PC{i}: {var:.3%}')\n",
    "\n",
    "pca2 = PCA(n_components=2, svd_solver='full')\n",
    "Z2 = pca2.fit_transform(X_z)            # → shape (n_samples, 2)\n",
    "\n",
    "# превращаем в DataFrame и подсоединяем метки class\n",
    "pca_df = pd.DataFrame(Z2, columns=['PC1', 'PC2'], index=X.index)\n",
    "pca_df['class'] = loans_df['class'].values\n",
    "\n",
    "pca2 = PCA(n_components=2, svd_solver='full')\n",
    "Z2 = pca2.fit_transform(X_z)            # → shape (n_samples, 2)\n",
    "\n",
    "# превращаем в DataFrame и подсоединяем метки class\n",
    "pca_df = pd.DataFrame(Z2, columns=['PC1', 'PC2'], index=X.index)\n",
    "pca_df['class'] = loans_df['class'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ad9c37984be1d7",
   "metadata": {},
   "source": [
    "### Task 1.2: Classifier Construction\n",
    "\n",
    "Use the dataset as a k-nearest neighbors (KNN) classifier. Use different values for the hyper-parameter *k*, e.g., three, four, five, and six, to fit a KNN classifier. Use different hyper-parameter values for the construction of the classifier.\n",
    "\n",
    "You may use the KNIME workflow to obtain the training data if you cannot complete the previous task. To export a CSV file with the preprocessed training data from the KNIME workflow, you have to change the *folder* field in the **Create File/Folder Variables** of the provided KNIME workflow in accordance with your directory hierarchy.\n",
    "\n",
    "See https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html for k-nearest neighbors in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c19ac58416c32b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 3: train accuracy = 0.9741, test accuracy = 0.9572\n",
      "k = 4: train accuracy = 0.9755, test accuracy = 0.9572\n",
      "k = 5: train accuracy = 0.9650, test accuracy = 0.9547\n",
      "k = 6: train accuracy = 0.9679, test accuracy = 0.9560\n",
      "\n",
      "Best k = 3, with test accuracy = 0.9572\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier  # KNN classifier algorithm\n",
    "from sklearn.model_selection import cross_val_score  # cross-validation function\n",
    "from sklearn.preprocessing import LabelEncoder       # label encoding (string → number)\n",
    "from sklearn.model_selection import train_test_split # splitting dataset into train and test\n",
    "from sklearn.metrics import accuracy_score           # metric: accuracy\n",
    "\n",
    "# Encode class labels: 'failure' → 0, 'ok' → 1\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "\"\"\"\n",
    "LabelEncoder:\n",
    "- fit(y): learns unique class labels and maps them to integers\n",
    "           Example: ['failure', 'ok'] → { 'failure': 0, 'ok': 1 }\n",
    "- transform(y): applies the mapping to the list y\n",
    "\"\"\"\n",
    "\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# 2. Split the dataset into training and test sets\n",
    "# test_size=0.3 → 30% of the data is reserved for testing\n",
    "# random_state=42 → fix random seed for reproducibility\n",
    "# stratify=y_encoded → keep the same proportion of classes (failure/ok) in train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=y_encoded\n",
    ")\n",
    "# Select different k values to try\n",
    "k_values = [3, 4, 5, 6]  # different numbers of neighbors to test\n",
    "\n",
    "best_k = None                  # best k value found\n",
    "best_test_accuracy = 0.0        # highest test accuracy found\n",
    "\n",
    "#Train and evaluate a KNN for each k\n",
    "\n",
    "for k in k_values:\n",
    "    # Create a KNN classifier with current k\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    \n",
    "    # Train the KNN model on the training set\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict the labels on the test set\n",
    "    y_pred = knn.predict(X_test)\n",
    "    \n",
    "    # Calculate training accuracy (how well the model fits the training data)\n",
    "    train_acc = accuracy_score(y_train, knn.predict(X_train))\n",
    "    \n",
    "    # Calculate testing accuracy (how well the model generalizes)\n",
    "    test_acc = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Print both training and testing accuracy\n",
    "    print(f\"k = {k}: train accuracy = {train_acc:.4f}, test accuracy = {test_acc:.4f}\")\n",
    "    \n",
    "    # Keep track of the best model based on test set performance\n",
    "    if test_acc > best_test_accuracy:\n",
    "        best_test_accuracy = test_acc\n",
    "        best_k = k\n",
    "print(f\"\\nBest k = {best_k}, with test accuracy = {best_test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf00a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebd992c2c7506b",
   "metadata": {},
   "source": [
    "### Task 1.3: Classifier Evaluation\n",
    "\n",
    "Conduct five-fold cross-validation to obtain estimates of the performance of the different knn classifiers.\n",
    "\n",
    "See https://scikit-learn.org/stable/modules/cross_validation.html for more information cross-validation using scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "84d91cde1e3963ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K=3, accuracy scores: [0.79301412 0.78825102 0.78841815 0.79226205 0.78917022], mean accuracy: 0.7902\n",
      "K=4, accuracy scores: [0.76451909 0.7613437  0.76067519 0.76568898 0.7604245 ], mean accuracy: 0.7625\n",
      "K=5, accuracy scores: [0.8062171  0.80479652 0.80128687 0.81231721 0.80396089], mean accuracy: 0.8057\n",
      "K=6, accuracy scores: [0.79426757 0.78774964 0.79084148 0.79627308 0.78917022], mean accuracy: 0.7917\n",
      "\n",
      "Best K = 5, with mean accuracy = 0.8057\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold   # Import for cross-validation\n",
    "from sklearn.neighbors import KNeighborsClassifier           # KNN classifier\n",
    "\n",
    "# Initialize best parameters\n",
    "\n",
    "best_k = None         # store the best number of neighbors\n",
    "best_score = 0.0      #store the best mean accuracy found\n",
    "\n",
    "\n",
    "# Setup 5-fold cross-validation\n",
    "# KFold: splits the data into 5 parts, shuffles before splitting, fixes random seed for reproducibility\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Try different values of k (number of neighbors)\n",
    "\n",
    "for k in k_values:\n",
    "    # Create a KNN classifier with k neighbors\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    \n",
    "    # Perform cross-validation:\n",
    "    # - split the data into 5 folds\n",
    "    # - train on 4 folds, test on the remaining 1 fold\n",
    "    # - repeat 5 times so each fold is used once for testing\n",
    "    # - scoring='accuracy' means we measure accuracy metric\n",
    "    scores = cross_val_score(knn, X_pca, y, cv=cv, scoring='accuracy')\n",
    "    \n",
    "    # Print individual fold scores and their mean\n",
    "    print(f\"K={k}, accuracy scores: {scores}, mean accuracy: {scores.mean():.4f}\")\n",
    "    \n",
    "    # Update best k if current model has higher mean accuracy\n",
    "    if scores.mean() > best_score:\n",
    "        best_score = scores.mean()\n",
    "        best_k = k\n",
    "\n",
    "print(f\"\\nBest K = {best_k}, with mean accuracy = {best_score:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34875424ad25a4cc",
   "metadata": {},
   "source": [
    "### Task 1.4: Deployment\n",
    "\n",
    "Export one of the previously constructed classifiers as a **pickle** file named \"classifier.pkl\".\n",
    "\n",
    "See https://scikit-learn.org/stable/model_persistence.html for options on model persistence and refer to the pickle module documentation.\n",
    "\n",
    "Install the **KNIME Python Integration** and **KNIME Conda Integration** (optional, if you use Anaconda/Conda) extensions in KNIME. Integrate the pickle file in the provided KNIME workflow and use your model to classify new observations. Note that you have to load the exported pickle file into the data directory of the KNIME workflow (inside the project's directory in your KNIME workspace).\n",
    "\n",
    "We will take the loans with unknown class from the original dataset and use the trained model to predict the class. Take a look at the preprocessing steps in the KNIME workflow. Notice that the preprocessing of the \"new\" data to be classified uses the same models for normalization and PCA than the training data, i.e., we use the normalization model and the PCA weight matrix obtained from the training data. Run the KNIME workflow and look at the obtained predictions in the output table of the **Python Script** node.\n",
    "\n",
    "**Note:** Take a look at the Python script classifiers. You have to be able to explain what happens there.\n",
    "\n",
    "Export the updated workflow and upload the file on Moodle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f2b128ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_pack keys: dict_keys(['scaler', 'pca', 'classifier', 'variable_means', 'cols_to_keep'])\n",
      "scaler: <class 'sklearn.preprocessing._data.StandardScaler'>\n",
      "pca: <class 'sklearn.decomposition._pca.PCA'>\n",
      "classifier: <class 'sklearn.neighbors._classification.KNeighborsClassifier'>\n",
      "\n",
      " model saved in data/classifier.pkl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Обучаем модель KNN на PCA-признаках (X_pca_df был создан ранее)\n",
    "final_knn = KNeighborsClassifier(n_neighbors=best_k)\n",
    "final_knn.fit(X_pca_df, y_encoded)\n",
    "\n",
    "# Упаковываем необходимые объекты для предсказания\n",
    "model_pack = {\n",
    "    'scaler':     scaler,      # обученный StandardScaler\n",
    "    'pca':        pca,         # обученный PCA\n",
    "    'classifier': final_knn    # обученный KNN на PC1 + PC2\n",
    "}\n",
    "\n",
    "model_pack = {\n",
    "    'scaler': scaler,                          # объект стандартизации\n",
    "    'pca': pca,                                # объект PCA\n",
    "    'classifier': final_knn,                 # финальный обученный классификатор\n",
    "    'variable_means': variable_means,          # средние значения признаков\n",
    "    'cols_to_keep': X_clean.columns.tolist()   # признаки, прошедшие отбор\n",
    "}\n",
    "\n",
    "# Информация для отладки\n",
    "print(\"model_pack keys:\", model_pack.keys())\n",
    "print(\"scaler:\", type(model_pack['scaler']))\n",
    "print(\"pca:\", type(model_pack['pca']))\n",
    "print(\"classifier:\", type(model_pack['classifier']))\n",
    "\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "with open(os.path.join(\"data\", \"classifier.pkl\"), 'wb') as f:\n",
    "    pickle.dump(model_pack, f)\n",
    "\n",
    "print(\"\\n model saved in data/classifier.pkl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbac508da08e6e0c",
   "metadata": {},
   "source": [
    "## Case 2: Truck Fleet Maintenance\n",
    "\n",
    "**Files**: trucks_training.csv; trucks_test.csv, trucks_full.csv\n",
    "\n",
    "A transportation company manages a fleet of trucks, each of which is equipped with hundreds of sensors that measure the operating conditions of several components while out on the road.\n",
    "\n",
    "The Air Pressure System (APS) is one of the components of interest. If a truck's APS is suspected to fail soon, the truck can be proactively called in for maintenance service at a relatively low but non-zero cost. Conversely, if a truck's APS is assumed to be working properly but does fail, the truck must be repaired in the field at relatively high cost.\n",
    "\n",
    "While the many sensors of a truck's APS do not indicate explicitly whether or not the APS will fail soon, we might be able to identify patterns that would allow for predicting the health of a truck's APS in advance of a failure. Using a suitable predictive model for APS failure would potentially allow the company to reduce maintenance costs by calling in trucks early, before a relatively costly field maintenance is required due to a failure.\n",
    "\n",
    "Your goal is to construct a predictive model for supporting the following decision:\n",
    "\n",
    "*Which trucks should be called in for APS maintenance service?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4c99826de9c492",
   "metadata": {},
   "source": [
    "### Task 2.1: Data Loading\n",
    "\n",
    "Use pandas to load the dataset **trucks_training.csv** of APS sensor measurements into a dataframe. Each observation describes the sensor measurements for a unique truck. The 170 predictor variables represent the 170 types of sensors. A *class* variable indicates whether a truck's APS has failed (**pos**) or not (**neg**). Rename the class labels: **neg** becomes **ok** and **pos** becomes **failure**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50e67d09273f9625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  class  aa_000 ab_000      ac_000 ad_000 ae_000 af_000 ag_000 ag_001 ag_002  \\\n",
      "0    ok   76698     na  2130706438    280      0      0      0      0      0   \n",
      "1    ok   33058     na           0     na      0      0      0      0      0   \n",
      "2    ok   41040     na         228    100      0      0      0      0      0   \n",
      "3    ok      12      0          70     66      0     10      0      0      0   \n",
      "4    ok   60874     na        1368    458      0      0      0      0      0   \n",
      "\n",
      "   ...   ee_002  ee_003  ee_004  ee_005  ee_006  ee_007  ee_008 ee_009 ef_000  \\\n",
      "0  ...  1240520  493384  721044  469792  339156  157956   73224      0      0   \n",
      "1  ...   421400  178064  293306  245416  133654   81140   97576   1500      0   \n",
      "2  ...   277378  159812  423992  409564  320746  158022   95128    514      0   \n",
      "3  ...      240      46      58      44      10       0       0      0      4   \n",
      "4  ...   622012  229790  405298  347188  286954  311560  433954   1218      0   \n",
      "\n",
      "  eg_000  \n",
      "0      0  \n",
      "1      0  \n",
      "2      0  \n",
      "3     32  \n",
      "4      0  \n",
      "\n",
      "[5 rows x 171 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd   # Import pandas library for data handling\n",
    "import numpy as np    # Import numpy library for numerical operations\n",
    "\n",
    "\n",
    "trucks_training_df = pd.read_csv('trucks_training.csv')\n",
    "\n",
    "#Replace class labels\n",
    "\n",
    "# In the 'class' column:\n",
    "# - replace 'neg' with 'ok'\n",
    "# - replace 'pos' with 'failure'\n",
    "trucks_training_df['class'] = trucks_training_df['class'].replace({\n",
    "    'neg': 'ok',\n",
    "    'pos': 'failure'\n",
    "})\n",
    "print(trucks_training_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa0ae3a9106955e",
   "metadata": {},
   "source": [
    "### Task 2.2: Data Understanding (I)\n",
    "\n",
    "Familiarize yourself with the dataset. In particular, look at the count and relative frequency of the observations for **failure** and **ok** classes, respectively. Furthermore, determine the number of missing values for each observation, and determine the number of missing values for each variable. Determine the variance of each variable. Look at the correlation between the class and the different predictor variables. Furthermore, look at the correlation between different predictor variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b312bbe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Absolute class counts:\n",
      "class\n",
      "ok         4916\n",
      "failure      84\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Relative class frequency:\n",
      "class\n",
      "ok         0.9832\n",
      "failure    0.0168\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Missing values per column (top 10):\n",
      "br_000    4113\n",
      "bq_000    4065\n",
      "bp_000    3994\n",
      "ab_000    3883\n",
      "cr_000    3883\n",
      "bo_000    3868\n",
      "bn_000    3654\n",
      "bm_000    3259\n",
      "bl_000    2219\n",
      "bk_000    1866\n",
      "dtype: int64\n",
      "\n",
      "Missing values per observation (summary stats):\n",
      "count    5000.000000\n",
      "mean       14.216000\n",
      "std        16.505109\n",
      "min         0.000000\n",
      "25%         8.000000\n",
      "50%         8.000000\n",
      "75%        14.000000\n",
      "max       168.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np          # For numerical operations\n",
    "import pandas as pd         # For working with dataframes\n",
    "import matplotlib.pyplot as plt  # For plotting graphs\n",
    "import seaborn as sns       # For advanced visualization\n",
    "\n",
    "# Check class balance: how many 'ok' and 'failure' examples\n",
    "\n",
    "\n",
    "# Count the absolute number of samples for each class\n",
    "class_counts = trucks_training_df['class'].value_counts()\n",
    "\n",
    "# Calculate the relative frequency (%) for each class\n",
    "class_freq = trucks_training_df['class'].value_counts(normalize=True)\n",
    "\n",
    "print(\" Absolute class counts:\")\n",
    "print(class_counts)\n",
    "\n",
    "print(\"\\nRelative class frequency:\")\n",
    "print(class_freq)\n",
    "\n",
    "\n",
    "# Replace literal 'na' strings with real np.nan values\n",
    "\n",
    "# Some missing values are stored as strings 'na' in the dataset.\n",
    "# Replace 'na' with actual np.nan (missing value marker)\n",
    "train_df = trucks_training_df.replace('na', np.nan)\n",
    "\n",
    "#Analyze missing values\n",
    "\n",
    "#  Missing values per column\n",
    "# Find how many missing values there are in each column\n",
    "# and sort columns by the number of missing values (descending)\n",
    "missing_per_col = train_df.isna().sum().sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nMissing values per column (top 10):\")\n",
    "print(missing_per_col.head(10))\n",
    "\n",
    "# 3Missing values per row\n",
    "# Find how many missing values there are in each row (observation)\n",
    "missing_per_row = train_df.isna().sum(axis=1)\n",
    "\n",
    "# Display summary statistics (mean, min, max, etc.) about missing values per observation\n",
    "print(\"\\nMissing values per observation (summary stats):\")\n",
    "print(missing_per_row.describe())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "566ad040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total # numeric variables: 170\n",
      "Completely empty columns  : 0\n",
      "\n",
      "Top-10 variables by variance\n",
      "ac_000    6.258477e+17\n",
      "dq_000    5.882853e+15\n",
      "eb_000    1.221873e+15\n",
      "du_000    1.473618e+14\n",
      "bb_000    1.076414e+14\n",
      "bx_000    1.065537e+14\n",
      "bv_000    1.061808e+14\n",
      "bu_000    1.061808e+14\n",
      "cq_000    1.061808e+14\n",
      "cc_000    9.363443e+13\n",
      "dtype: float64\n",
      "\n",
      "NaN count in y: 0\n",
      "\n",
      "Strongest correlations with 'class' (abs value, top 10)\n",
      "bj_000   -0.589156\n",
      "aa_000   -0.561595\n",
      "bt_000   -0.559848\n",
      "bb_000   -0.557472\n",
      "ci_000   -0.555589\n",
      "bv_000   -0.549438\n",
      "bu_000   -0.549438\n",
      "cq_000   -0.549438\n",
      "al_000   -0.549361\n",
      "ap_000   -0.549356\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SUPERSONIC\\anaconda3\\Lib\\site-packages\\numpy\\lib\\function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "C:\\Users\\SUPERSONIC\\anaconda3\\Lib\\site-packages\\numpy\\lib\\function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 5×5 block of the correlation matrix\n",
      "          aa_000    ab_000    ac_000    ad_000    ae_000\n",
      "aa_000  1.000000 -0.028441 -0.052762  0.081942  0.037248\n",
      "ab_000 -0.028441  1.000000 -0.014283 -0.021965  0.015125\n",
      "ac_000 -0.052762 -0.014283  1.000000  0.042419  0.001112\n",
      "ad_000  0.081942 -0.021965  0.042419  1.000000  0.011808\n",
      "ae_000  0.037248  0.015125  0.001112  0.011808  1.000000\n"
     ]
    }
   ],
   "source": [
    "# Prepare X (numeric predictors) and y (binary target)\n",
    "y = train_df['class'].map({'failure': 0, 'ok': 1})         # target: 0/1\n",
    "X = train_df.drop(columns='class').apply(pd.to_numeric,    # convert every sensor to float\n",
    "                                         errors='coerce')\n",
    "\n",
    "\n",
    "#Basic feature diagnostics\n",
    "\n",
    "print(f\"\\nTotal # numeric variables: {X.shape[1]}\")\n",
    "print(f\"Completely empty columns  : {X.isna().all().sum()}\")\n",
    "\n",
    "# list empty columns, if any\n",
    "empty_cols = X.columns[X.isna().all()].tolist()\n",
    "if empty_cols:\n",
    "    print(\"Empty columns:\", empty_cols)\n",
    "\n",
    "# Variance of every variable\n",
    "variances = X.var()                      # pandas var() skips NaN by default\n",
    "print(\"\\nTop-10 variables by variance\")\n",
    "print(variances.sort_values(ascending=False).head(10))\n",
    "\n",
    "print(\"\\nNaN count in y:\", y.isna().sum())\n",
    "\n",
    "\n",
    "# Correlation with the class (feature importance proxy)\n",
    "# fill remaining NaN with median to avoid bias\n",
    "\n",
    "X_imp = X.dropna(axis=1, how='all')      # drop columns that are 100 % NaN\n",
    "X_imp = X_imp.fillna(X_imp.median())     # simple median imputation\n",
    "\n",
    "corr_with_class = X_imp.corrwith(y)      # Pearson by default\n",
    "print(\"\\nStrongest correlations with 'class' (abs value, top 10)\")\n",
    "print(corr_with_class.reindex(corr_with_class.abs()\n",
    "                              .sort_values(ascending=False).index).head(10))\n",
    "\n",
    "#Correlation between predictor variables\n",
    "#    – useful to spot multicollinearity before PCA or model\n",
    "corr_matrix = X_imp.corr()\n",
    "\n",
    "print(\"\\nFirst 5×5 block of the correlation matrix\")\n",
    "print(corr_matrix.iloc[:5, :5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a850aeec38db433d",
   "metadata": {},
   "source": [
    "### Task 2.3: Data Cleaning\n",
    "\n",
    "Filter out (i.e., remove) low-/zero-variance variables. Filter out (i.e., remove) variables and rows with too many missing values. Otherwise, impute missing values with the variable mean. For each of those variables, keep the variable means; we will need them for the preprocessing after deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2209c8a4cf47ba09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns before dropping missing-heavy ones: 170\n",
      "Columns after dropping those with >50% missing: 162\n",
      "Rows after dropping those with >50% missing: 4962\n",
      "Any NaNs left after imputation? 0\n",
      "\n",
      " Shape before cleaning: (4962, 162)\n",
      " Shape after cleaning : (4962, 160)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# - converted to numeric\n",
    "# - no 'class' column\n",
    "# - contains NaN values\n",
    "# - shape: (n_samples, n_features)\n",
    "\n",
    "# Drop variables (columns) with too many missing values\n",
    "# define \"too many\" as >50% missing (can be adjusted if needed)\n",
    "\n",
    "\n",
    "missing_ratio_per_column = X.isna().mean()  # fraction of NaN per column\n",
    "columns_to_keep = missing_ratio_per_column[missing_ratio_per_column <= 0.5].index\n",
    "X = X[columns_to_keep]\n",
    "\n",
    "print(f\"Columns before dropping missing-heavy ones: {X.shape[1] + len(missing_ratio_per_column[missing_ratio_per_column > 0.5])}\")\n",
    "print(f\"Columns after dropping those with >50% missing: {X.shape[1]}\")\n",
    "\n",
    "# Drop rows (observations) with too many missing values\n",
    "# define \"too many\" as >50% missing in that row\n",
    "\n",
    "missing_ratio_per_row = X.isna().mean(axis=1)\n",
    "X = X.loc[missing_ratio_per_row <= 0.5]\n",
    "\n",
    "print(f\"Rows after dropping those with >50% missing: {X.shape[0]}\")\n",
    "\n",
    "# Impute remaining missing values with the mean of each column\n",
    "# Save those means — we will use the same ones during deployment!\n",
    "\n",
    "# Compute and store means (excluding NaN)\n",
    "variable_means = X.mean()\n",
    "\n",
    "# Replace remaining NaNs with the column mean\n",
    "X_filled = X.fillna(variable_means)\n",
    "\n",
    "print(f\"Any NaNs left after imputation? {X_filled.isna().sum().sum()}\")  # should be 0\n",
    "\n",
    "#  Remove low-variance features (e.g., var < 0.01)\n",
    "# features do not contribute much to distinguishing the target\n",
    "\n",
    "\n",
    "selector = VarianceThreshold(threshold=0.01)  # Remove features with very little variation\n",
    "X_reduced = selector.fit_transform(X_filled)\n",
    "\n",
    "# Get names of the features that survived\n",
    "remaining_columns = X_filled.columns[selector.get_support()]\n",
    "X_clean = pd.DataFrame(X_reduced, columns=remaining_columns)\n",
    "\n",
    "# Summary: check shapes before and after cleaning\n",
    "\n",
    "print(\"\\n Shape before cleaning:\", X.shape)      # e.g., (5000, 170)\n",
    "print(\" Shape after cleaning :\", X_clean.shape)  # e.g., (4850, 120)\n",
    "\n",
    "# keep these for future deployment:\n",
    "# - `variable_means` → for imputing test data the same way\n",
    "# - `selector` → for applying same feature reduction to new data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3021d8131d5f57e",
   "metadata": {},
   "source": [
    "### Task 2.4: Data Understanding (II)\n",
    "\n",
    "Represent the dataset in principal component form. Normalize the predictor variables, use them to compute a weight matrix, and apply the weight matrix to the predictor variables to compute the principal components of the observations. Note that the transformed dataset still represents exactly the same observations, though they are now expressed in terms of different predictor variables.\n",
    "\n",
    "Identify the variables (principal components) that comprise just over 50 % of the total variance. To identify those variables, obtain the variance and the cumulative variance per principal component. Relate the variance and the cumulative variance per principal component with the total variance (sum of variance) to obtain the proportion of the variance and the cumulative proportion of the variance per principal component.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f7f79bb177714773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of components explaining ≥ 50% of total variance: 7\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Top 10 principal components with highest explained variance:\n",
      "  Principal Component  Variance Explained (%)  \\\n",
      "0                 PC1                0.308027   \n",
      "1                 PC2                0.056480   \n",
      "2                 PC3                0.038297   \n",
      "3                 PC4                0.036284   \n",
      "4                 PC5                0.026909   \n",
      "5                 PC6                0.022560   \n",
      "6                 PC7                0.021429   \n",
      "7                 PC8                0.017660   \n",
      "8                 PC9                0.016816   \n",
      "9                PC10                0.016312   \n",
      "\n",
      "   Cumulative Variance Explained (%)  \n",
      "0                           0.308027  \n",
      "1                           0.364507  \n",
      "2                           0.402803  \n",
      "3                           0.439088  \n",
      "4                           0.465996  \n",
      "5                           0.488557  \n",
      "6                           0.509986  \n",
      "7                           0.527646  \n",
      "8                           0.544462  \n",
      "9                           0.560774  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Normalize the dataset using Z-score normalization\n",
    "#  ensure each feature has mean=0 and standard deviation=1,\n",
    "#    which is required before applying PCA.\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_clean)  # X_clean = cleaned dataset (no NaNs, only numeric)\n",
    "\n",
    "# Perform Principal Component Analysis\n",
    "#    PCA transforms the dataset into uncorrelated principal components.\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "#  Variance explained by each principal component\n",
    "explained_variance_ratio = pca.explained_variance_ratio_  # share of variance explained per PC\n",
    "\n",
    "# Cumulative variance across components\n",
    "\n",
    "cumulative_variance = np.cumsum(explained_variance_ratio)  # running total of explained variance\n",
    "\n",
    "# Identify the number of components explaining ≥ 50% of total variance\n",
    "\n",
    "num_components_50 = np.argmax(cumulative_variance >= 0.5) + 1\n",
    "print(f\" Number of components explaining ≥ 50% of total variance: {num_components_50}\")\n",
    "\n",
    "# Plot cumulative variance curve (Scree plot)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance,\n",
    "         marker='o', linestyle='--', color='blue')\n",
    "plt.axhline(y=0.5, color='red', linestyle='-', label='50% Threshold')\n",
    "plt.title('Cumulative Explained Variance by Principal Components')\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Variance Explained')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Create summary table for each principal component\n",
    "\n",
    "pca_results = pd.DataFrame({\n",
    "    'Principal Component': [f'PC{i+1}' for i in range(len(explained_variance_ratio))],\n",
    "    'Variance Explained (%)': explained_variance_ratio,\n",
    "    'Cumulative Variance Explained (%)': cumulative_variance\n",
    "})\n",
    "\n",
    "\n",
    "print(\"🔍 Top 10 principal components with highest explained variance:\")\n",
    "print(pca_results.head(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d834c037c20b4f",
   "metadata": {},
   "source": [
    "### Task 2.5: Data Preprocessing\n",
    "\n",
    "Keep only the principal components that comprise just over 50 % of the total variance. Keep all the **ok** observations from the dataset and perform bootstrapping, i.e., random sampling with replacement, of the minority (**failure**) class to obtain a balanced dataset with as many **failure** observations as **ok** observations. Finally, split the dataset into *training set* and *validation set*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a00c12ed9a08ba1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of principal components retained: 7\n",
      "Balanced dataset shape: (9832, 8)\n",
      "Training set shape 80%: (7865, 7)\n",
      "Validation set shape 20%: (1967, 7)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# 1 Keep only the principal components that explain just over 50% of total variance\n",
    "# Use the first `num_components_50` principal components from the previously computed PCA\n",
    "\n",
    "X_pca_reduced = X_pca[:, :num_components_50]  # shape = (n_samples, num_components_50)\n",
    "y_clean = y.loc[X_clean.index]\n",
    "# Create a DataFrame to hold principal components along with the class labels\n",
    "df_pca = pd.DataFrame(X_pca_reduced, columns=[f'PC{i+1}' for i in range(num_components_50)])\n",
    "df_pca['class'] = y.loc[df_pca.index].values # y contains binary encoded labels: 1 = ok, 0 = failure\n",
    "\n",
    "# 2 Balance the dataset\n",
    "# - Keep all 'ok' class samples (majority)\n",
    "# - Upsample the minority class 'failure' using bootstrapping (sampling with replacement)\n",
    "\n",
    "df_ok = df_pca[df_pca['class'] == 1]        # All \"ok\" observations\n",
    "df_failure = df_pca[df_pca['class'] == 0]   # All \"failure\" observations\n",
    "\n",
    "# Bootstrapping: randomly resample failure observations with replacement\n",
    "df_failure_upsampled = df_failure.sample(n=len(df_ok), replace=True, random_state=42)\n",
    "\n",
    "# Combine balanced dataset and shuffle the rows\n",
    "df_balanced = pd.concat([df_ok, df_failure_upsampled], axis=0).sample(frac=1, random_state=42)\n",
    "\n",
    "# 3 - Split the balanced dataset into training and validation sets\n",
    "# Use stratified split to preserve class ratio in both sets\n",
    "X_bal = df_balanced.drop(columns='class')  # features (principal components)\n",
    "y_bal = df_balanced['class']               # target labels (0 = failure, 1 = ok)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_bal, y_bal,\n",
    "    test_size=0.2,            # 20% goes to validation set\n",
    "    stratify=y_bal,           # preserve class ratio\n",
    "    random_state=42           # ensure reproducibility\n",
    ")\n",
    "\n",
    "print(f\" Number of principal components retained: {num_components_50}\")\n",
    "print(f\"Balanced dataset shape: {df_balanced.shape}\")\n",
    "print(f\"Training set shape 80%: {X_train.shape}\")\n",
    "print(f\"Validation set shape 20%: {X_val.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb4e3191dbe58e6",
   "metadata": {},
   "source": [
    "### Task 2.6: Classifier Training\n",
    "\n",
    "Use scikit-learn to train a support vector machine (SVM) classifier on the training set. Set the *probability* parameter to **true** in order for the classifier to obtain probabilities for the observations to belong to a certain class.\n",
    "\n",
    "See https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC for more information on training an SVM classifier in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "56479203fecd9222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix (failure=0, ok=1):\n",
      " [[936  48]\n",
      " [ 52 931]]\n",
      "Accuracy: 94.92% of all predictions are correct\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95       984\n",
      "           1       0.95      0.95      0.95       983\n",
      "\n",
      "    accuracy                           0.95      1967\n",
      "   macro avg       0.95      0.95      0.95      1967\n",
      "weighted avg       0.95      0.95      0.95      1967\n",
      "\n",
      "\n",
      "🔍 Predicted Probabilities (first 10 samples):\n",
      "   Prob_failure (class 0)  Prob_ok (class 1)  Predicted_class  Actual_class\n",
      "0                0.991809           0.008191                0             0\n",
      "1                0.992957           0.007043                0             0\n",
      "2                0.957148           0.042852                0             0\n",
      "3                0.957148           0.042852                0             0\n",
      "4                0.028537           0.971463                1             1\n",
      "5                0.957218           0.042782                0             0\n",
      "6                0.957176           0.042824                0             0\n",
      "7                0.026099           0.973901                1             1\n",
      "8                0.049393           0.950607                1             1\n",
      "9                0.038765           0.961235                1             1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import pandas as pd  # Needed to display probabilities nicely\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Initialize the Support Vector Machine (SVM) classifier\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "# Create an SVM classifier with probability estimation enabled\n",
    "# `probability=True` allows us to later use `.predict_proba()` to get class probabilities\n",
    "# This is useful when we want to apply custom probability thresholds or plot ROC curves\n",
    "svm_clf = SVC(probability=True, random_state=42)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Train the classifier on the training data\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "# Fit the classifier on the training set\n",
    "# X_train: training feature matrix (e.g., PC1, PC2, ..., PC7)\n",
    "# y_train: training target labels (0 = failure, 1 = ok)\n",
    "svm_clf.fit(X_train, y_train)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Predict on the validation set\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "# Predict class labels (0 or 1) on the validation data\n",
    "y_pred = svm_clf.predict(X_val)\n",
    "\n",
    "# Predict class probabilities\n",
    "# y_proba[:, 0] = probability of class 0 (failure)\n",
    "# y_proba[:, 1] = probability of class 1 (ok)\n",
    "y_proba = svm_clf.predict_proba(X_val)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Evaluate the model's performance\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "# Confusion matrix interpretation:\n",
    "#         predicted\n",
    "# actual   0    1\n",
    "#       0 [TN, FP]\n",
    "#       1 [FN, TP]\n",
    "# the model predicts failures\n",
    "print(\"Confusion matrix (failure=0, ok=1):\\n\", confusion_matrix(y_val, y_pred))\n",
    "\n",
    "# Example explanation:\n",
    "# 936  - True Negative  - real failure predicted correctly\n",
    "# 52   - False Negative - failure classified as OK\n",
    "# 48   - False Positive - OK classified as failure\n",
    "# 931  - True Positive  - real OK predicted correctly\n",
    "\n",
    "# Accuracy: proportion of correct predictions\n",
    "print(f\"Accuracy: {accuracy_score(y_val, y_pred)*100:.2f}% of all predictions are correct\")\n",
    "\n",
    "# Detailed classification report\n",
    "# Includes precision, recall, f1-score, and support for each class\n",
    "print(\"\\nClassification report:\\n\", classification_report(y_val, y_pred))\n",
    "# Support: number of real samples for each class\n",
    "# F1-score: harmonic mean between precision and recall\n",
    "\n",
    "# The model is now reasonably balanced between the two classes\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Output Predicted Probabilities for each validation sample\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "# Create a DataFrame showing:\n",
    "# - probability of failure (class 0)\n",
    "# - probability of ok (class 1)\n",
    "# - predicted class\n",
    "# - actual class\n",
    "proba_df = pd.DataFrame({\n",
    "    'Prob_failure (class 0)': y_proba[:, 0],\n",
    "    'Prob_ok (class 1)': y_proba[:, 1],\n",
    "    'Predicted_class': y_pred,\n",
    "    'Actual_class': y_val.reset_index(drop=True)  # Align indices\n",
    "})\n",
    "\n",
    "# Display the first 10 rows to see the probabilities\n",
    "print(\"\\n🔍 Predicted Probabilities (first 10 samples):\")\n",
    "print(proba_df.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f644b27d42587782",
   "metadata": {},
   "source": [
    "### Task 2.7: Classifier Tuning\n",
    "\n",
    "Train SVM classifiers with different hyper-parameter settings, e.g., setting the *probability* parameter to **false** (but try also varying the values for other hyper-parameters).\n",
    "\n",
    "See https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC for more information on the hyper-parameters.\n",
    "\n",
    "Use different cutoff values for classifying the observations in the validation set as **failure** or **ok**, e.g., classify an observation as **failure** if the predicted probability of that observation belonging to the class is at least 25 %, 50 %, 75 %, 90 %.\n",
    "\n",
    "Compare precision, recall, and accuracy of using classifiers with different hyper-parameters and cutoff values. Use the **validation set** to test the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "37f40bdbd2efe81a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "kernel",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "C",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "gamma",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "cutoff",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "precision",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "recall",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "accuracy",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "505f3b8c-6e18-4ffa-a64e-940d2859ce03",
       "rows": [
        [
         "0",
         "rbf",
         "1.0",
         "scale",
         "0.25",
         "0.9516129032258065",
         "0.960325534079349",
         "0.9557702084392475"
        ],
        [
         "1",
         "rbf",
         "1.0",
         "scale",
         "0.5",
         "0.9507186858316222",
         "0.9420142421159715",
         "0.9466192170818505"
        ],
        [
         "2",
         "rbf",
         "1.0",
         "scale",
         "0.75",
         "0.950207468879668",
         "0.9318413021363174",
         "0.9415353329944077"
        ],
        [
         "3",
         "rbf",
         "1.0",
         "scale",
         "0.9",
         "0.9489904357066951",
         "0.9084435401831129",
         "0.9298423995932893"
        ],
        [
         "4",
         "rbf",
         "0.5",
         "scale",
         "0.25",
         "0.9512690355329949",
         "0.953204476093591",
         "0.9522114895780376"
        ],
        [
         "5",
         "rbf",
         "0.5",
         "scale",
         "0.5",
         "0.9506172839506173",
         "0.9399796541200407",
         "0.945602440264362"
        ],
        [
         "6",
         "rbf",
         "0.5",
         "scale",
         "0.75",
         "0.95",
         "0.9277721261444557",
         "0.9395017793594306"
        ],
        [
         "7",
         "rbf",
         "0.5",
         "scale",
         "0.9",
         "0.9487726787620064",
         "0.9043743641912513",
         "0.9278088459583121"
        ],
        [
         "8",
         "rbf",
         "1.0",
         "0.1",
         "0.25",
         "1.0",
         "0.9949135300101729",
         "0.9974580579562786"
        ],
        [
         "9",
         "rbf",
         "1.0",
         "0.1",
         "0.5",
         "1.0",
         "0.9938962360122076",
         "0.9969496695475343"
        ],
        [
         "10",
         "rbf",
         "1.0",
         "0.1",
         "0.75",
         "1.0",
         "0.9928789420142421",
         "0.99644128113879"
        ],
        [
         "11",
         "rbf",
         "1.0",
         "0.1",
         "0.9",
         "1.0",
         "0.9908443540183113",
         "0.9954245043213015"
        ],
        [
         "12",
         "linear",
         "1.0",
         "auto",
         "0.25",
         "0.9123661148977604",
         "0.953204476093591",
         "0.9308591764107779"
        ],
        [
         "13",
         "linear",
         "1.0",
         "auto",
         "0.5",
         "0.9507186858316222",
         "0.9420142421159715",
         "0.9466192170818505"
        ],
        [
         "14",
         "linear",
         "1.0",
         "auto",
         "0.75",
         "0.9494204425711275",
         "0.9165818921668362",
         "0.9339095068632435"
        ],
        [
         "15",
         "linear",
         "1.0",
         "auto",
         "0.9",
         "0.9725714285714285",
         "0.8657171922685656",
         "0.9206914082358922"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 16
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kernel</th>\n",
       "      <th>C</th>\n",
       "      <th>gamma</th>\n",
       "      <th>cutoff</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rbf</td>\n",
       "      <td>1.0</td>\n",
       "      <td>scale</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.951613</td>\n",
       "      <td>0.960326</td>\n",
       "      <td>0.955770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rbf</td>\n",
       "      <td>1.0</td>\n",
       "      <td>scale</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.950719</td>\n",
       "      <td>0.942014</td>\n",
       "      <td>0.946619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rbf</td>\n",
       "      <td>1.0</td>\n",
       "      <td>scale</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.950207</td>\n",
       "      <td>0.931841</td>\n",
       "      <td>0.941535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rbf</td>\n",
       "      <td>1.0</td>\n",
       "      <td>scale</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.948990</td>\n",
       "      <td>0.908444</td>\n",
       "      <td>0.929842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rbf</td>\n",
       "      <td>0.5</td>\n",
       "      <td>scale</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.951269</td>\n",
       "      <td>0.953204</td>\n",
       "      <td>0.952211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rbf</td>\n",
       "      <td>0.5</td>\n",
       "      <td>scale</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.950617</td>\n",
       "      <td>0.939980</td>\n",
       "      <td>0.945602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>rbf</td>\n",
       "      <td>0.5</td>\n",
       "      <td>scale</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.927772</td>\n",
       "      <td>0.939502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rbf</td>\n",
       "      <td>0.5</td>\n",
       "      <td>scale</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.948773</td>\n",
       "      <td>0.904374</td>\n",
       "      <td>0.927809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>rbf</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.994914</td>\n",
       "      <td>0.997458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>rbf</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.993896</td>\n",
       "      <td>0.996950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>rbf</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.992879</td>\n",
       "      <td>0.996441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>rbf</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.990844</td>\n",
       "      <td>0.995425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>linear</td>\n",
       "      <td>1.0</td>\n",
       "      <td>auto</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.912366</td>\n",
       "      <td>0.953204</td>\n",
       "      <td>0.930859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>linear</td>\n",
       "      <td>1.0</td>\n",
       "      <td>auto</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.950719</td>\n",
       "      <td>0.942014</td>\n",
       "      <td>0.946619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>linear</td>\n",
       "      <td>1.0</td>\n",
       "      <td>auto</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.949420</td>\n",
       "      <td>0.916582</td>\n",
       "      <td>0.933910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>linear</td>\n",
       "      <td>1.0</td>\n",
       "      <td>auto</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.972571</td>\n",
       "      <td>0.865717</td>\n",
       "      <td>0.920691</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    kernel    C  gamma  cutoff  precision    recall  accuracy\n",
       "0      rbf  1.0  scale    0.25   0.951613  0.960326  0.955770\n",
       "1      rbf  1.0  scale    0.50   0.950719  0.942014  0.946619\n",
       "2      rbf  1.0  scale    0.75   0.950207  0.931841  0.941535\n",
       "3      rbf  1.0  scale    0.90   0.948990  0.908444  0.929842\n",
       "4      rbf  0.5  scale    0.25   0.951269  0.953204  0.952211\n",
       "5      rbf  0.5  scale    0.50   0.950617  0.939980  0.945602\n",
       "6      rbf  0.5  scale    0.75   0.950000  0.927772  0.939502\n",
       "7      rbf  0.5  scale    0.90   0.948773  0.904374  0.927809\n",
       "8      rbf  1.0    0.1    0.25   1.000000  0.994914  0.997458\n",
       "9      rbf  1.0    0.1    0.50   1.000000  0.993896  0.996950\n",
       "10     rbf  1.0    0.1    0.75   1.000000  0.992879  0.996441\n",
       "11     rbf  1.0    0.1    0.90   1.000000  0.990844  0.995425\n",
       "12  linear  1.0   auto    0.25   0.912366  0.953204  0.930859\n",
       "13  linear  1.0   auto    0.50   0.950719  0.942014  0.946619\n",
       "14  linear  1.0   auto    0.75   0.949420  0.916582  0.933910\n",
       "15  linear  1.0   auto    0.90   0.972571  0.865717  0.920691"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "# Balanced dataset\n",
    "df_pca = pd.DataFrame(X_pca_reduced, columns=[f'PC{i+1}' for i in range(num_components_50)])\n",
    "df_pca['class'] = y.loc[df_pca.index].values\n",
    "df_ok = df_pca[df_pca['class'] == 1]\n",
    "df_failure = df_pca[df_pca['class'] == 0]\n",
    "df_failure_upsampled = df_failure.sample(n=len(df_ok), replace=True, random_state=42)\n",
    "df_balanced = pd.concat([df_ok, df_failure_upsampled], axis=0).sample(frac=1, random_state=42)\n",
    "X_bal = df_balanced.drop(columns='class')\n",
    "y_bal = df_balanced['class']\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_bal, y_bal, test_size=0.2, stratify=y_bal, random_state=42)\n",
    "\n",
    "# Define different hyperparameter settings\n",
    "# kernel → shape of the boundary.\n",
    "# 'linear'\tStraight line separation. Best when classes are linearly separable.\n",
    "# 'rbf'\tNonlinear boundary. Works well when classes are intertwined.\n",
    "#'poly'\tPolynomial curves; flexibility depends on degree.\n",
    "\n",
    "# C — penalty for misclassification\n",
    "# Controls the tradeoff between maximizing margin and minimizing training error.\n",
    "# Lower C → softer margin, allows some misclassification.\n",
    "# Higher C → strict margin, tries to classify training points perfectly.\n",
    "\n",
    "# gamma — how sensitive the model is to individual data points.\n",
    "# Used in nonlinear kernels like 'rbf':\n",
    "# Controls how far each point's influence reaches.\n",
    "# Low gamma → broader influence (smoother boundaries).\n",
    "# High gamma → sharp, reactive boundaries (risk of overfitting).\n",
    "\n",
    "svm_configs = [\n",
    "    {\"kernel\": \"rbf\", \"C\": 1.0, \"gamma\": \"scale\"},\n",
    "    {\"kernel\": \"rbf\", \"C\": 0.5, \"gamma\": \"scale\"},\n",
    "    {\"kernel\": \"rbf\", \"C\": 1.0, \"gamma\": 0.1},\n",
    "    {\"kernel\": \"linear\", \"C\": 1.0},\n",
    "]\n",
    "\n",
    "# Different cutoffs for classification\n",
    "cutoffs = [0.25, 0.5, 0.75, 0.9]\n",
    "\n",
    "results = []\n",
    "\n",
    "for config in svm_configs:\n",
    "    model = SVC(probability=True, **config, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_proba = model.predict_proba(X_val)[:, 1]\n",
    "    for cutoff in cutoffs:\n",
    "        y_pred_custom = (y_proba >= cutoff).astype(int)\n",
    "        precision = precision_score(y_val, y_pred_custom, zero_division=0)\n",
    "        recall = recall_score(y_val, y_pred_custom)\n",
    "        accuracy = accuracy_score(y_val, y_pred_custom)\n",
    "        results.append({\n",
    "            \"kernel\": config.get(\"kernel\"),\n",
    "            \"C\": config.get(\"C\"),\n",
    "            \"gamma\": config.get(\"gamma\", \"auto\"),\n",
    "            \"cutoff\": cutoff,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"accuracy\": accuracy\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "from IPython.display import display\n",
    "display(results_df) \n",
    "\n",
    "# The recall shows what fraction of all true failing machines we actually predicted as failing.\n",
    "# model number 0 is the best for us  - max recall and precision is also the best\n",
    "# here precision is also the best - small share of false alarms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af98c0aeea0d31e7",
   "metadata": {},
   "source": [
    "### Task 2.8: Evaluation\n",
    "\n",
    "Now train the classifier on the concatenation of training and validation set using your choice of hyper-parameter settings and remember the cutoff value (if the probabilistic SVM classifier was your choice).\n",
    "\n",
    "Load the **trucks_test.csv** dataset for the evaluation. To match the original representation of the dataset used to train the classifier, perform the same preprocessing as for the training. In particular, rename the class labels in the new dataset, impute missing values in the **new** dataset with variable means **previously calculated from the original dataset**, and remove variables that were also previously removed. Use the PCA weight matrix **previously calculated from the original dataset** and transform the new dataset into principal component form. Keep only the same variables as in the original dataset. You do not need to balance the dataset.\n",
    "\n",
    "We are going to evaluate the classifier based on the cost savings with respect to the baseline scenarios. Assume that the cost of maintenance service per truck is 100 euros, the cost of field repair per truck that has failed is 5,000 euros.\n",
    "\n",
    "Compare the performance of the baseline scenarios---i.e., calling in all trucks for maintenance service and calling in no trucks for maintenance service, respectively---with the chosen classifier in terms of accuracy, precision, and recall as well as total maintenance costs in euros. *Calling in all trucks for maintenance service* corresponds to a classifier that always predicts **failure** for any observation whereas *calling in no trucks for maintenance service* corresponds to a classifier that always predicts **ok** for any observation.\n",
    "\n",
    "Note that in the following, we treat **ok** trucks as the positive class and **failure** trucks as the negative class. We use the following terminology:\n",
    "- The proportion of incorrectly predicted trucks as **failure** from among those that are actually good is the **false negative rate**.\n",
    "- The proportion of correctly predicted trucks as **failure** from among those that are actually **failure** is the **true negative rate**.\n",
    "- The proportion of incorrectly predicted trucks as good from among those that are actually **failure** is the **false positive rate**.\n",
    "\n",
    "The total maintenance costs are as follows:\n",
    "\n",
    "*costs* = (*costs per maintenance service* × *# trucks for necessary maintenance*) +\n",
    "          (*costs per maintenance service* × *# trucks for unnecessary maintenance*) +\n",
    "          (*costs per field repair* × *# trucks for field repair*) +\n",
    "          (0 × *# other trucks*)\n",
    "\n",
    "where\n",
    "- *# trucks for necessary maintenance* = *true negative rate* × *# trucks actually failing*,\n",
    "- *# trucks for unnecessary maintenance* = *false negative rate* × *# trucks actually not failing*,\n",
    "- *# trucks for field repair* = *false positive rate* × *# trucks actually failing*, and\n",
    "- *# other trucks* = *true positive rate* × *# trucks actually not failing*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a1b792d4ac1f4e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16000 entries, 0 to 15999\n",
      "Columns: 171 entries, class to eg_000\n",
      "dtypes: int64(1), object(170)\n",
      "memory usage: 20.9+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,\n",
       "   class  aa_000 ab_000 ac_000 ad_000 ae_000 af_000 ag_000 ag_001 ag_002  ...  \\\n",
       " 0    ok      60      0     20     12      0      0      0      0      0  ...   \n",
       " 1    ok      82      0     68     40      0      0      0      0      0  ...   \n",
       " 2    ok   66002      2    212    112      0      0      0      0      0  ...   \n",
       " 3    ok   59816    NaN   1010    936      0      0      0      0      0  ...   \n",
       " 4    ok    1814    NaN    156    140      0      0      0      0      0  ...   \n",
       " \n",
       "    ee_002  ee_003  ee_004  ee_005   ee_006  ee_007  ee_008 ee_009 ef_000  \\\n",
       " 0    1098     138     412     654       78      88       0      0      0   \n",
       " 1    1068     276    1620     116       86     462       0      0      0   \n",
       " 2  495076  380368  440134  269556  1315022  153680     516      0      0   \n",
       " 3  540820  243270  483302  485332   431376  210074  281662   3232      0   \n",
       " 4    7646    4144   18466   49782     3176     482      76      0      0   \n",
       " \n",
       "   eg_000  \n",
       " 0      0  \n",
       " 1      0  \n",
       " 2      0  \n",
       " 3      0  \n",
       " 4      0  \n",
       " \n",
       " [5 rows x 171 columns])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "test_df = pd.read_csv(\"trucks_test.csv\")\n",
    "\n",
    "# 1. Rename the class labels: 'neg' → 'ok', 'pos' → 'failure'\n",
    "test_df['class'] = test_df['class'].replace({'neg': 'ok', 'pos': 'failure'})\n",
    "\n",
    "# 2. Replace 'na' strings with actual np.nan\n",
    "test_df = test_df.replace('na', np.nan)\n",
    "\n",
    "# Display basic info and head\n",
    "test_df_info = test_df.info()\n",
    "test_df_head = test_df.head()\n",
    "\n",
    "test_df_info, test_df_head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b34383df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final SVM model trained.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SUPERSONIC\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but SVC was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report for Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.91      0.54       375\n",
      "           1       1.00      0.97      0.98     15625\n",
      "\n",
      "    accuracy                           0.96     16000\n",
      "   macro avg       0.69      0.94      0.76     16000\n",
      "weighted avg       0.98      0.96      0.97     16000\n",
      "\n",
      "\n",
      "Total Maintenance Costs: 4221600 euros\n",
      "\n",
      "Accuracy: 0.96\n",
      "Precision: 1.00\n",
      "Recall: 0.97\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "\n",
    "test_df = pd.read_csv('trucks_test.csv')\n",
    "\n",
    "#Replace class labels to match training\n",
    "test_df['class'] = test_df['class'].replace({'neg': 'ok', 'pos': 'failure'})\n",
    "\n",
    "#Replace 'na' with np.nan\n",
    "test_df = test_df.replace('na', np.nan)\n",
    "\n",
    "#Separate features and target\n",
    "y_test_actual = test_df['class'].map({'failure': 0, 'ok': 1})\n",
    "X_test = test_df.drop(columns='class')\n",
    "\n",
    "# Leave only columns kept during training\n",
    "#Make sure you have 'cols_to_keep' loaded from your training\n",
    "X_test = X_test[remaining_columns]\n",
    "\n",
    "#Convert to numeric\n",
    "X_test = X_test.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "#Fill missing values with training means\n",
    "X_test = X_test.fillna(variable_means)\n",
    "\n",
    "#Standardize using training scaler\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "#PCA transform using training PCA\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "# Keep only first num_components_50 principal components\n",
    "X_test_pca = X_test_pca[:, :num_components_50]\n",
    "\n",
    "# Prepare the final training data\n",
    "\n",
    "# Use balanced and cleaned training data (already PCA-reduced)\n",
    "# Make sure you use X_bal and y_bal that were prepared correctly\n",
    "X_final_train = X_bal.reset_index(drop=True)\n",
    "y_final_train = y_bal.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Fill missing values if needed (usually not needed if preprocessed properly)\n",
    "X_final_train = X_final_train.fillna(X_final_train.mean())\n",
    "\n",
    "# Train final SVM classifier\n",
    "\n",
    "svm_classifier_final = SVC(C=1.0, kernel='linear', probability=True, random_state=42)\n",
    "svm_classifier_final.fit(X_final_train, y_final_train)\n",
    "\n",
    "print(\"Final SVM model trained.\")\n",
    "\n",
    "# Make predictions on the test set\n",
    "\n",
    "# Predict class probabilities\n",
    "y_test_proba = svm_classifier_final.predict_proba(X_test_pca)\n",
    "\n",
    "# Apply cutoff\n",
    "cutoff = 0.5\n",
    "y_test_pred = (y_test_proba[:, 1] >= cutoff).astype(int)  # 1 = ok, 0 = failure\n",
    "\n",
    "\n",
    "#Evaluate the predictions\n",
    "print(\"\\nClassification Report for Test Set:\")\n",
    "print(classification_report(y_test_actual, y_test_pred))\n",
    "\n",
    "# Confusion matrix\n",
    "tn, fp, fn, tp = confusion_matrix(y_test_actual, y_test_pred).ravel()\n",
    "\n",
    "\n",
    "# Cost calculation\n",
    "\n",
    "cost_per_maintenance = 100   # euros\n",
    "cost_per_repair = 5000       # euros\n",
    "\n",
    "necessary_maintenance = tp   # trucks correctly identified as failure\n",
    "unnecessary_maintenance = fp # trucks wrongly predicted as failure\n",
    "field_repairs = fn           # trucks wrongly predicted as ok\n",
    "\n",
    "# Total maintenance cost\n",
    "total_costs = (\n",
    "    (cost_per_maintenance * necessary_maintenance) +\n",
    "    (cost_per_maintenance * unnecessary_maintenance) +\n",
    "    (cost_per_repair * field_repairs)\n",
    ")\n",
    "\n",
    "print(f\"\\nTotal Maintenance Costs: {total_costs} euros\")\n",
    "\n",
    "\n",
    "#Additional Metrics\n",
    "\n",
    "accuracy = (tp + tn) / (tn + fp + fn + tp)\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "print(f\"\\nAccuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e19631ddd4b9e",
   "metadata": {},
   "source": [
    "How do you judge the usefulness of the predictive model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db87999e2653abd",
   "metadata": {},
   "source": [
    "High recall for \"failure\" trucks (91%) is very good.\n",
    "We catch most of the trucks that are about to fail\n",
    "Very important in this task, because missing a failure is very expensive (5000€)\n",
    "\n",
    "Very high precision for \"ok\" trucks (100%)\n",
    "Almost never wrongly labeling a broken truck as OK\n",
    "\n",
    "Total maintenance cost: 4.2 million euros — lower than blindly calling all trucks or ignoring all trucks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b189cfb3f137d7b",
   "metadata": {},
   "source": [
    "### Task 2.9: Data Preprocessing and Modeling (Revisited)\n",
    "\n",
    "Go back to the *Data Preprocessing* and *Modeling* stages of the CRISP-DM. Use the **trucks_full.csv** dataset for data mining. You are now free to use different preprocessing techniques (you are *not* limited to principal component analysis) and different algorithms (you are *not* limited to SVM classifiers). Evaluate the performance of each classifier. Choose suitable sampling/validation strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a4468001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Dataset Info ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 60000 entries, 0 to 59999\n",
      "Columns: 171 entries, class to eg_000\n",
      "dtypes: int64(1), object(170)\n",
      "memory usage: 78.3+ MB\n",
      "None\n",
      "\n",
      "--- First 5 Rows ---\n",
      "  class  aa_000 ab_000      ac_000 ad_000 ae_000 af_000 ag_000 ag_001 ag_002  \\\n",
      "0   neg   76698     na  2130706438    280      0      0      0      0      0   \n",
      "1   neg   33058     na           0     na      0      0      0      0      0   \n",
      "2   neg   41040     na         228    100      0      0      0      0      0   \n",
      "3   neg      12      0          70     66      0     10      0      0      0   \n",
      "4   neg   60874     na        1368    458      0      0      0      0      0   \n",
      "\n",
      "   ...   ee_002  ee_003  ee_004  ee_005  ee_006  ee_007  ee_008 ee_009 ef_000  \\\n",
      "0  ...  1240520  493384  721044  469792  339156  157956   73224      0      0   \n",
      "1  ...   421400  178064  293306  245416  133654   81140   97576   1500      0   \n",
      "2  ...   277378  159812  423992  409564  320746  158022   95128    514      0   \n",
      "3  ...      240      46      58      44      10       0       0      0      4   \n",
      "4  ...   622012  229790  405298  347188  286954  311560  433954   1218      0   \n",
      "\n",
      "  eg_000  \n",
      "0      0  \n",
      "1      0  \n",
      "2      0  \n",
      "3     32  \n",
      "4      0  \n",
      "\n",
      "[5 rows x 171 columns]\n",
      "\n",
      "--- Missing Values per Column ---\n",
      "class         0\n",
      "aa_000        0\n",
      "ab_000    46329\n",
      "ac_000     3335\n",
      "ad_000    14861\n",
      "          ...  \n",
      "ee_007      671\n",
      "ee_008      671\n",
      "ee_009      671\n",
      "ef_000     2724\n",
      "eg_000     2723\n",
      "Length: 171, dtype: int64\n",
      "\n",
      "--- Shape After Dropping NaNs ---\n",
      "(591, 171)\n",
      "\n",
      "--- Numeric Columns ---\n",
      "['aa_000']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "trucks_full = pd.read_csv(\"trucks_full.csv\")\n",
    "\n",
    "# Show the structure of the dataset\n",
    "print(\"\\n--- Dataset Info ---\")\n",
    "print(trucks_full.info())\n",
    "\n",
    "# Show first 5 rows to understand the data\n",
    "print(\"\\n--- First 5 Rows ---\")\n",
    "print(trucks_full.head())\n",
    "\n",
    "#Data cleaning and preparation\n",
    "\n",
    "# Replace target labels: 'neg' → 'ok', 'pos' → 'failure'\n",
    "trucks_full['class'] = trucks_full['class'].replace({'neg': 'ok', 'pos': 'failure'})\n",
    "\n",
    "# Replace 'na' strings with proper np.nan values\n",
    "trucks_full = trucks_full.replace('na', np.nan)\n",
    "\n",
    "# Check missing values per column\n",
    "print(\"\\n--- Missing Values per Column ---\")\n",
    "print(trucks_full.isna().sum())\n",
    "\n",
    "# Drop rows that contain any missing values\n",
    "trucks_full = trucks_full.dropna()\n",
    "\n",
    "# Check the new shape after dropping missing rows\n",
    "print(\"\\n--- Shape After Dropping NaNs ---\")\n",
    "print(trucks_full.shape)\n",
    "\n",
    "# Select only numeric columns for modeling\n",
    "numeric_cols = trucks_full.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(\"\\n--- Numeric Columns ---\")\n",
    "print(numeric_cols)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2ac0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Logistic Regression Results =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          ok       0.89      0.99      0.94       104\n",
      "     failure       0.67      0.13      0.22        15\n",
      "\n",
      "    accuracy                           0.88       119\n",
      "   macro avg       0.78      0.56      0.58       119\n",
      "weighted avg       0.86      0.88      0.85       119\n",
      "\n",
      "Confusion matrix:\n",
      " [[103   1]\n",
      " [ 13   2]]\n",
      "\n",
      "===== Random Forest (300 Trees) Results =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          ok       0.92      0.93      0.93       104\n",
      "     failure       0.50      0.47      0.48        15\n",
      "\n",
      "    accuracy                           0.87       119\n",
      "   macro avg       0.71      0.70      0.71       119\n",
      "weighted avg       0.87      0.87      0.87       119\n",
      "\n",
      "Confusion matrix:\n",
      " [[97  7]\n",
      " [ 8  7]]\n",
      "\n",
      "5-Fold CV Accuracy | Logistic Regression: 0.878 ± 0.012\n",
      "                  | Random Forest       : 0.846 ± 0.022\n"
     ]
    }
   ],
   "source": [
    "# Feature-target separation\n",
    "\n",
    "# X = features (input variables)\n",
    "X = trucks_full[numeric_cols]\n",
    "\n",
    "# y = target variable (binary classification: ok → 0, failure → 1)\n",
    "y = trucks_full['class'].map({'ok': 0, 'failure': 1})\n",
    "\n",
    "# Missing value imputation (optional, to be extra safe)\n",
    "\n",
    "\n",
    "# Fill missing values in features with column means\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed = pd.DataFrame(imputer.fit_transform(X), columns=numeric_cols, index=X.index)\n",
    "\n",
    "# Splitting the dataset into train and validation sets\n",
    "\n",
    "\n",
    "# Use 80% for training, 20% for validation\n",
    "# Stratify = keep the same proportion of classes in both sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_imputed, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Feature scaling (standardization)\n",
    "\n",
    "# Scale features: mean = 0, standard deviation = 1\n",
    "scaler = StandardScaler()\n",
    "X_train_s = pd.DataFrame(scaler.fit_transform(X_train), columns=numeric_cols, index=X_train.index)\n",
    "X_val_s = pd.DataFrame(scaler.transform(X_val), columns=numeric_cols, index=X_val.index)\n",
    "\n",
    "# Model training and evaluation\n",
    "\n",
    "\n",
    "# 7-A. Logistic Regression model\n",
    "lr_clf = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr_clf.fit(X_train_s, y_train)\n",
    "\n",
    "# Predict on validation set\n",
    "y_pred_lr = lr_clf.predict(X_val_s)\n",
    "\n",
    "# Evaluation metrics for Logistic Regression\n",
    "print(\"\\n===== Logistic Regression Results =====\")\n",
    "print(classification_report(y_val, y_pred_lr, target_names=['ok', 'failure']))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_val, y_pred_lr))\n",
    "\n",
    "\n",
    "#  Random Forest model\n",
    "rf_clf = RandomForestClassifier(\n",
    "    n_estimators=300,           # number of trees in the forest\n",
    "    random_state=42,            # reproducibility\n",
    "    n_jobs=-1,                  # use all CPU cores\n",
    "    class_weight=\"balanced\"     # handle imbalanced classes\n",
    ")\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on validation set\n",
    "y_pred_rf = rf_clf.predict(X_val)\n",
    "\n",
    "# Evaluation metrics for Random Forest\n",
    "print(\"\\n===== Random Forest (300 Trees) Results =====\")\n",
    "print(classification_report(y_val, y_pred_rf, target_names=['ok', 'failure']))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_val, y_pred_rf))\n",
    "\n",
    "# Simple cross-validation (5-fold)\n",
    "\n",
    "# Create stratified k-fold cross-validator\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Cross-validation scores for Logistic Regression\n",
    "lr_cv_scores = cross_val_score(lr_clf, X_imputed, y, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Cross-validation scores for Random Forest\n",
    "rf_cv_scores = cross_val_score(rf_clf, X_imputed, y, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Show average and standard deviation of accuracy\n",
    "print(f\"\\n5-Fold CV Accuracy | Logistic Regression: {lr_cv_scores.mean():.3f} ± {lr_cv_scores.std():.3f}\")\n",
    "print(f\"                  | Random Forest       : {rf_cv_scores.mean():.3f} ± {rf_cv_scores.std():.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
